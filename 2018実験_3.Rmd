---
title: 情報工学実験３：パターン認識
date: "`r Sys.Date()`"
author: "小坪 琢人"
output:
  rmdformats::readthedown:
    highlight: kate
    md_extensions: -ascii_identifiers
    number_sections: true
    theme: united
#output:
 # ioslides_presentation:
 # toc: true
---

```{r knitr_init, echo=FALSE, cache=FALSE}
## Global options
#options(max.print="500")
knitr:::opts_chunk$set(cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
knitr:::opts_knit$set(width=75)
```

# 実験準備

## ディレクトリーを変更する.

USB上ではデータを扱わない方がいいです. 配布されたフォルダをデスクトップ上にダウンロードし, 最後に自分のUSBに戻しましょう. ディレクトリ名は各自変更してください.

```{r setup, echo=FALSE}
# mac用
setwd("~/Desktop/実験資料part3")
# windows用
# setwd("C:User/Desktop/情報工学実験３")
```

## パッケージのインストール

学科PCは初期化されるため, その都度インストールする必要があります. 以下のコードを実行することで, 必要なパッケージをインストールすることができます. また, 警告文などでパッケージが存在しないと言われた場合, パッケージのインストールが正しく行われているか確認しましょう.

```{r echo=FALSE,eval=FALSE}
# echo=FALSE: markdownに表示されない, eval = FALSE : markdownで実行されない
install.packages("tidyverse")
install.packages("data.table")
install.packages("moments")
install.packages("MASS")
install.packages("class")
install.packages("kernlab")
install.packages("xtable")
install.packages("knitr")
install.packages("readr") 
```

## パッケージの読み込み
 
パッケージはインストールして, 読み込むことで使用できるようになります. 自分のノートPC使用している場合, インストールは一度でいいですが, 読み込みは毎回行う必要があります.

```{r}
# 必要なパッケージを読み込む.
library(tidyverse)
library(data.table)
library(moments)
library(MASS) #eqscplot, データ
library(class) # k最近傍法
library(kernlab) #カーネル法
library(xtable) # latexで表を出力
library(knitr) # kable
```

## データの読み込み

前回配布されたデータを今回も用いる.

```{r}
train <- read.csv("csv/pima_train.csv") # 訓練用データ
test <- read.csv("csv/pima_correct.csv") # テスト用データ
```

# 第三回の目的

第三回では, ロジスティック回帰とサポートベクターマシーンについて実験を行う. 
今回の基礎となる, ベイズの定理および点と直線の距離の公式について復習する. 簡単に説明するが, もしわからない場合は必ず各自復習し理解した上でレポートに取り組むこと.

# 復習

## ベイズの定理について(復習) 

事象$A$が起こるという条件の下で, $k$種類の事象$B$が起こるとする. このとき条件付き確率$P(B|A)$は次の式から求められる.

$$
P(B|A) = \frac{P(A \cap B)}{P(A)}
$$

ここで, 乗法定理$P(A\cap B) = P(B) \times P(A|B)$を代入する.

$$
P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(B)P(A|B)}{P(A)}
$$

得られた式をベイズの定理と呼ぶ. ここで, $P(B)$を事前確率, $P(B|A)$を事後確率と呼ぶ. 解釈としては, もともと$B$が起きる確率は事前確率$P(B)$であったが, $A$が起きるという情報を得た元では事後確率$P(B|A)$と更新されると考える.

## 条件付き確率に関する問題 {.tabset .tabset-fade}

### 問題

1から3の目が黄色で塗られており, 4から6の目は水色で塗られているさいころがある. このさいころを投げて水色の目が出た時, この目が偶数である確率を求めよ.

### 解答

水色の目が出る事象を$A$, 偶数の目が出る事象を$B$とすると, 求める確率は$P(B|A)$となる. $A\cap B$となる事象は出る目が$\{4,6\}$の場合なので,

$$
P(A \cap B) = \frac{2}{6}
$$

以上を用いて, 次のように確率を求める.

$$
P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{\frac{2}{6}}{\frac{3}{6}} = \frac{2}{3}
$$


# 点と直線の距離について(復習)

2次元における直線$ax+by+c=0$と座標$(x_1,y_1)$の点と直線の距離は,

$$
d = \frac{|ax_1 + by_1 + c|}{\sqrt{a^2 + b^2}}
$$

で表される. 

## 点と平面の距離の公式を求めよ{.tabset .tabset-fade}

### 問題

上述した点と直線の距離の公式を元に, 一般化して点と平面の距離の公式を求める.
(ヒント: 係数ベクトル$\bf{w}$, 特徴ベクトル$\bf{x_i}$, バイアス項$b$とする)


### 回答

係数ベクトル$\bf{w}$, 特徴ベクトル$\bf{x_i}$, バイアス項$b$として, 

$$
d = \frac{|\bf{w}^T \bf{x_i} + b|}{ \Vert  \bf{w} \Vert}
$$

と表すことができる.

# ロジスティック回帰

線形識別関数$y = \bf{w}^T \bf{x}$は, 識別境界から離れるに従って線形に上昇する. ロジスティック回帰は, 関数値を区間(0,1)に制限し, 確率的な解釈を可能にする. 

2クラス問題を考える. データ$\bf{x}$の下で, クラス$C_1$の事後確率$P(C_1 | \bf{x} )$は
$$
P(C_1 | \bf{x} ) = \frac{ P( \bf{x} |C_1 ) P(C_1)}{P( \bf{x} |C_1 ) P(C_1) + P( \bf{x} |C_2) P(C_2) }
$$

となる. ここで式の説明を行う. $P(C_1|\bf{x})$は任意のデータ$\bf{x}$の下での, クラス1に所属する確率.  $P(\bf{x}|C_1)$はクラス1に所属すると仮定した場合のデータの尤もらしさ(尤度)を表す. 

$$
a = \ln \frac{ P( \bf{x} |C_1 ) P(C_1)}{P( \bf{x} |C_2) P(C_2) }
$$

と置けば, 

$$
P(C_1 | \bf{x} ) =  \frac{1}{1 + \exp(-a)} = \sigma(a)
$$
と表すことができる. 関数$\sigma(a)$を$\textbf{ロジスティック関数}$と呼ぶ.

事後確率の比をオッズ, その対数を対数オッズという. 

$$
a = \ln \frac{ P( \bf{x} |C_1 ) P(C_1)}{P( \bf{x} |C_2) P(C_2) }
$$

## ピマ・インディアンデータのロジスティック回帰

ピマ・インディアンデータとは7種の特徴と測定してから5年後に糖尿病を発症したか否かを表す. ７つの特徴と疾病の有無について関係性をプロットする.

特徴:npreg, glu, bp, skin, bmi, ped, age, 結果:type(糖尿病を発症したor発症していない)

```{r logis.a}
pairs(train[,-8],pch=21, bg=c(109,34)[factor(train$type)])
# pch:プロットの形状, bg=109:青, bg=34:赤
```


`type`をその他７変数でロジスティック回帰を行う

```{r logis.b}
logit.fit1 <- glm(type ~ npreg + glu + bp + skin + bmi + ped + age, family = "binomial", data=train)
summary(logit.fit1) 
#Estimates:推定された各変数の係数パラメータ, AIC:情報量基準
```

[glm関数についての説明ページ](https://www.marketechlabo.com/r-glm-libraries/)

この出力結果から, 線形識別関数は

$$
\log\left( \frac{\exp (w^T x)}{ 1+\exp (w^T x)}\right)
=-9.1892 + 0.0853x_1 + 0.0373 x_2 -0.0236 x_3 + 0.0040x_4+ 0.0862 x_5 + 1.0408 x_6 + 0.0457 x_7
$$

と推定される.

学習データから, 誤り率と分類評価指標を求めてみよう.　下の結果から, 識別境界を事後確率が0.5としたときの誤り率は$20.1\%$であった.

```{r logis.c}
#元データの数値を用いて, 得られたパラメータにより線形識別関数を計算する.
pred.ln.logit <- predict(logit.fit1) 
pred.logit <- exp(pred.ln.logit)/(1+ exp(pred.ln.logit)) 

# 全てのデータにおける"type"を"No"とする 
pred.logit.type <- rep("No", 332)
# 線形識別関数を用いて0.5より大きい場合を"Yes"とする. 
pred.logit.type[pred.logit >0.5] <- "Yes"
#横軸を真値, 縦軸を予測値としてクロス集計表を作成する.
table( train$type, pred.logit.type)
#集計表の対角成分つまり, 的中したデータ数を元に, 誤り率を算出する.
error.rate <- 1-sum(diag(table( train$type, pred.logit.type)))/332
error.rate
```

事後確率の値を変化させて, 識別境界を変化させたときのROCのプロット.

```{r logis.roc}
#データ形成
ROC_ <- data.frame(value=pred.logit,answer=train$type)
colnames(ROC_) <- c('value','answer')
#出力値順に並べ替える
ROC_ <- ROC_[order(ROC_$value),]

#そのままだと計算が面倒なのでNoを0 Yesを1に変形する
levels(ROC_$answer) <- c(0,1)
#"answer"列のデータをfactor形式からnumeric形式に変更する
ROC_$answer <- ROC_$answer %>% 
  as.character() %>% 
  as.numeric()
#0,1の個数を計測する
table(ROC_$answer)

#最初にYesの人間が出現するところから
first <- which(ROC_$answer == 1)[1]
ROC_ <- ROC_[first:332,]

#本当はYesで、閾値をずらしたときに、予測したなかで正しかった割合
true_positive <- cumsum(ROC_$answer)/sum(ROC_$answer) #cumsum:累積和
# 予測が正しくない割合
false_positive <- cumsum(ROC_$answer - 1) / sum(ROC_$answer - 1)

# 予測が正しい割合と正しくない割合のデータでデータフレームを形成する.
plot_d <- data.frame(true_positive = true_positive,false_positive = false_positive)
ggplot(plot_d, aes(x=true_positive,y=false_positive)) + # ggplotはデータ, x軸, y軸を指定する
  geom_line() + # x軸, y軸を元に直線を引く
  theme_bw() # おまけ
```

## 実行例6.4 非線形基底関数による変換とロジスティク回帰による識別

ここでは, 2クラスのロジスティック回帰を用いて非線形基底関数の効用を示す. 実行例6.3で, 2次元判別空間に写像されたアヤメデータをsetosaとvirginicaを１クラスにまとめて線形分離不可能な２クラス問題にし, ロジスティック回帰で求めた識別境界を示す. ロジスティック回帰では, 事後確率が0.5となる等高線が識別境界となる. 図からわかるように, 線形関数では精度良く分類することはできない.

そこで非線形基底関数を使った非線形特徴写像を考える. 非線形特徴写像はいろいろなものが考えられるが, ここでは次のガウス核関数

$$
f(\bf{x}) =\exp ( -\alpha (\bf{x} - \bf{\mu} )^T \bf{\Sigma}^{-1} (\bf{x} - \bf{\mu} )) 
$$

を用いる.

# サポートベクトルマシン

サポートベクターマシンは, 最大マージンを実現する2クラス線形識別関数の学習法である. 最大マージンとは, 学習データの中で最も他のクラスと近い位置にいるもの(サポートベクトル)を基準として, その二つのデータについて, それぞれユークリッド距離が最も大きくなるような位置に識別境界を定めることである.

クラスラベル付き学習データの集合を$\mathcal{D}_L = \{ (t_i, \bf{x}_i)\},~(i=1,\ldots, N)$とする. $t_i = \{ -1,1\}$は教師データであり, 学習データ$\bf{x} \in \mathcal{R}^d$がどちらのクラスに属するのかを指定する. 

線形識別関数のマージンを$\kappa$とすれば, すべての学習データで

$$
| \bf{w}^T \bf{x}_i + b| \geq \kappa
$$

が成り立つ. ここで,先ほど示した点と平面の公式の分母である$\Vert  \bf{w} \Vert$は両辺にかけて消去している, その結果得られた値をマージン$\kappa$としている?
係数ベクトルとバイアス項をマージン$\kappa$で正規化したものを改めて$\bf{w}$と$b$と置けば, 線形識別関数は

$$
\begin{aligned}
t_i = +1~\textrm{の場合}, ~~~\bf{w}^T \bf{x}_i  + b \geq 1, \\
t_i = -1~\textrm{の場合}, ~~~\bf{w}^T \bf{x}_i  + b \leq -1
\end{aligned}
$$

となる. この場合分けは$t_i( \bf{w}^T \bf{x}_i + b ) \geq 1$とまとめることができる. クラス間マージンは, 各クラスのデータを$\bf{w}$の方向へ射影した長さの差の最小値(つまり$t_i = +1$のとき$\bf{w}^T \bf{x}_i  + b = 1$となり, $t_i = -1$のとき$\bf{w}^T \bf{x}_i  + b = -1$となるとき)

$$
\begin{eqnarray}
\rho (\bf{w}, b) &=& \min_{x \in C_{y =+1} } \frac{\bf{w}^T \bf{x}}{ \Vert  \bf{w} \Vert}
-
\max_{x \in C_{y =-1} } \frac{\bf{w}^T \bf{x}}{ \Vert  \bf{w} \Vert} \\ 
&=& \frac{1-b}{ \Vert  \bf{w} \Vert} - \frac{-1-b}{ \Vert  \bf{w} \Vert} \\
&=& \frac{2}{ \Vert  \bf{w} \Vert}
\end{eqnarray}
$$

で与えられる. 最適な超平面の式を$\bf{w}_0^T\bf{x} + b_0=0$とすれば, この超平面は最大クラス間マージン$\rho(\bf{w}_0, b_0) = \max_{w} \rho(\bf{w},b)$を与える. 従って, 最適識別超平面は, $t_i(\bf{w}^T \bf{x}_i+ b)\geq 1$の制約下で, $\bf{w}$のノルムを最小にする解$\bf{w}_0 = \min  \Vert  \bf{w} \Vert$としてもとめることができる.

`train`との`glu`と`bmi`を用いた2次元データをSVMで識別してみる. RBFカーネルを用いた結果を示す. RBFカーネルにはコストパラメータ$C$とカーネルパラメータ$\sigma$が存在する.動径基底関数の広がりを表すパラメータを$\sigma=0.2, 0.4, 0.8, 2.0,4.0$に設定し, $\alpha_i$の上限値を与えるパラメータ$C$の値を変えたときの訓練誤差(学習データの再代入誤り率)と,  ホールドアウト法による汎化誤差を示している. （教科書と違います）　

## 実行例8.1 ピマ・インディアンデータデのSVMによる識別

```{r ex8.1.a}
## 動径基底関数を作成する. Create a kernel function using the build in rbfdot function
rbf <- rbfdot(sigma=0.1)
rbf

## train a bound constraint support vector machine
pima.model <- ksvm(type~ bmi+glu ,data=train,type="C-svc",
                  kernel=rbf,C=10,prob.model=TRUE)

pima.model

## predict mail type on the test set
pred.tr <- predict(pima.model) # 訓練データに対する実行
pred.te <- predict(pima.model,test) #テストデータに対する実行

## 誤り率を計算する
1-sum(diag(table ( pred.tr, train$type) ))/332

1-sum(diag(table ( pred.te, test$type) ))/200

plot(pima.model)
```


```{r ex8.1.b}
#2つのパラメータを定める関数
svm.rbf <-function(sig,Const){
  rbf <- rbfdot(sigma=sig)
  ## train a bound constraint support vector machine
  pima.model <- ksvm(type~ bmi+glu ,data=train,type="C-svc",
                     kernel=rbf,C=Const,prob.model=TRUE)
  pred.tr<- predict(pima.model) 
  pred.te <- predict(pima.model,test)
  ## 誤り率を計算する
  error.tr <- 1-sum(diag(table ( pred.tr, train$type) ))/332
  error.te <- 1-sum(diag(table ( pred.te, test$type) ))/200
  return(list(error.te=error.te, error.tr = error.tr))
}

svm.rbf(sig=0.4,Const=1)
```

$\sigma$や$C$を変えながら訓練誤差と汎化誤差の振る舞い振いを調べる.

```{r ex8.1.c}
Sig <- c(0.2,0.4,0.8,4)
Const <- c(1,10, 100,1000,10000)

#sapply関数:svm.rdf関数に対して,Sig[i]のデータとConstの5つの数値を適用する.得られる結果はList形式で与えられる.
#svm.rdf関数では訓練データに対する誤り率, テストデータに対する誤り率を計算するため各outputは10個のListとなる.

out.s1 <- sapply(Const,svm.rbf, sig=Sig[1] )
out.s2 <- sapply(Const,svm.rbf, sig=Sig[2] )
out.s3 <- sapply(Const,svm.rbf, sig=Sig[3] )
out.s4 <- sapply(Const,svm.rbf, sig=Sig[4] )
result <- rbind(out.s1,out.s2,out.s3,out.s4)

matplot(0:4,t(result), type="o", xlab="log10(C))", ylab="error rate")
```


$\sigma$の値が小さくなるとカーネル関数が狭くなり, 個々のサポートベクトルが独立に識別境界を決めるようになるので, 複雑な識別境界を構成できるようになる. その半面, 学習データに強く適合してしまうため, 汎化誤差は大きくなる.

$C$の値が大きくなるほど, $\alpha_i$の値が大きくなり, 識別関数の線形結合係数が大きな値を取れるので, 複雑な識別局面が構成できてしまう結果, やはり学習データに強く適合するため汎化誤差が大きくなる. 汎化誤差が一番小さな$C=10, \sigma=0.4$, $C=100, \sigma=0.2$が最適なパラメータであると考えられる. 

汎化誤差が最小となった場合と, 訓練誤差が最小になった場合のサポートベクトルの分布を図に示す.

```{r ex8.1.d}
rbf <- rbfdot(sigma=0.4)
pima.model1 <- ksvm(type~ bmi+glu ,data=train,type="C-svc",
                  kernel=rbf,C=10,prob.model=TRUE)
plot(pima.model1,data= train)
table( predict(pima.model1, train), train$type)

rbf <- rbfdot(sigma=4)
pima.model2 <- ksvm(type~ bmi+glu ,data=train,type="C-svc",
                  kernel=rbf,C=10000,prob.model=TRUE)
plot(pima.model2,data=train)
table( predict(pima.model2, train), train$type)
```

# レポート課題
 
## ロジスティック回帰

今回の例では`train`データの7変数を用いて, ロジスティック回帰を行なったが, 幾つの変数を用いるべきか考察せよ.
必ず理由を記載すること. 用いたコードも添付すること.

## ROC曲線

ROC曲線について調べたことを記入せよ. 解釈の仕方など, 今回の結果を踏まえて作成しても良い.

## サポートベクトルマシン

今回の例では`train`の`glu`と`bmi`の2変数を用いて, サポートベクトルマシンによるニ値分類を行なったが, 他の変数を用いて同様の検証を行い, 考察せよ. その際どの変数, パラメータを用いたかを記載し, 必ず理由を記載すること.
いくつかのパターンを実行し, 何らかの評価指標を用いて選定した場合にはそれらを全て載せても構わない. ただし表などを用いてわかりやすくまとめること. 用いたコードも添付すること.

# レポートの構成

1. 目的（A4一枚程度）
      - ロジスティック回帰およびサポートベクトルマシンについて
2. 授業内課題(1~2)
      - 課題内容, 結果
3. レポート課題(1~3)
      - 課題内容, 必要に応じて手順, 結果, 考察
4. 感想（重要）

# 注意点

- 質問対応時間

    - 月曜16:00-17:30 (仮)

- レポート構成について

    - 配布したコードをそのまま貼らないでください.
    - 必ず自分の言葉で記入してください.

