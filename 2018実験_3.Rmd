---
title: 情報工学実験３：パターン認識
date: "`r Sys.Date()`"
author: "小坪 琢人"
output:
  rmdformats::readthedown:
    highlight: kate
    md_extensions: -ascii_identifiers
    number_sections: true
    theme: united
    #code_folding: hide
---

```{r knitr_init, echo=FALSE, cache=FALSE}
## Global options
#options(max.print="500")
knitr:::opts_chunk$set(cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
knitr:::opts_knit$set(width=75)
```

```{r setup, echo=FALSE}
# set working directory
setwd("~/Desktop/情報工学実験３")
```

```{r, echo=FALSE}
# load packages
library(tidyverse)
library(data.table)
library(moments)
library(MASS) #eqscplot, データ
library(class) # k最近傍法
library(kernlab) #カーネル法
library(xtable) # latexで表を出力
library(knitr) # kable
```

# ベイズの定理について(復習) 

事象$A$が起こるという条件の下で, $k$種類の事象$B$が起こるとする. このとき条件付き確率$P(B|A)$は次の式から求められる.

$$
P(B|A) = \frac{P(A \cap B)}{P(A)}
$$

ここで, 乗法定理$P(A\cap B) = P(B) \times P(A|B)$を代入する.

$$
P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(B)P(A|B)}{P(A)}
$$

得られた式をベイズの定理と呼ぶ. ここで, $P(B)$を事前確率, $P(B|A)$を事後確率と呼ぶ. 解釈としては, もともと$B$が起きる確率は事前確率$P(B)$であったが, $A$が起きるという情報を得た元では事後確率$P(B|A)$と更新されると考える.

# ロジスティック回帰

線形識別関数$y = \bf{w}^T \bf{x}$は, 識別境界から離れるに従って線形に上昇する. ロジスティック回帰は, 関数値を区間(0,1)に制限し, 確率的な解釈を可能にする. 

2クラス問題を考える. データ$\bf{x}$の下で, クラス$C_1$の事後確率$P(C_1 | \bf{x} )$は
$$
P(C_1 | \bf{x} ) = \frac{ P( \bf{x} |C_1 ) P(C_1)}{P( \bf{x} |C_1 ) P(C_1) + P( \bf{x} |C_2) P(C_2) }
$$
であるが, 
$$
a = \ln \frac{ P( \bf{x} |C_1 ) P(C_1)}{P( \bf{x} |C_2) P(C_2) }
$$
と置けば, 
$$
P(C_1 | \bf{x} ) =  \frac{1}{1 + \exp(-a)} = \sigma(a)
$$
と表すことができる. 関数$\sigma(a)$を\textbf{ロジスティック関数}と呼ぶ.

事後確率の比をオッズ, その対数を対数オッズという. 
$$
a = \ln \frac{ P( \bf{x} |C_1 ) P(C_1)}{P( \bf{x} |C_2) P(C_2) }
$$


## ピマ・インディアンデータのロジスティック回帰

ピマ・インディアンデータとは7種の特徴と測定してから5年後に糖尿病を発症したか否かを表す.

特徴:npreg, glu, bp, skin, bmi, ped, age, 結果:type(Yes or No)

７つの特徴と疾病の有無の関係をプロットする.

```{r logis.a}
pairs(Pima.tr[,-8],pch=21, bg=c(109,34)[factor(Pima.tr$type)])
```


`type`をその他７変数でロジスティック回帰を行う

```{r logis.b}
logit.fit1 <- glm(type ~ npreg + glu + bp + skin + bmi + ped + age, family = "binomial", data=Pima.tr)
summary(logit.fit1)
```

[glm関数について](https://www.marketechlabo.com/r-glm-libraries/)

この出力結果から, 線形識別関数は

$$
\log\left( \frac{\exp (w^T x)}{ 1+\exp (w^T x)}\right)
=-9.7731 + 0.1032x_1 + 0.0321 x_2 -0.0048 x_3 -0.0019x_4+ 0.0838 x_5 + 1.8204 x_6 + 0.04118 x_7
$$

と推定される.

学習データから, 誤り率と分類評価指標を求めてみよう.　下の結果から, 識別境界を事後確率が0.5としたときの誤り率は$22.5\%$であった.

```{r logis.c}
#元データの数値を用いて, 得られたパラメータにより線形識別関数を計算する.
pred.ln.logit <- predict(logit.fit1) 
pred.logit <- exp(pred.ln.logit)/(1+ exp(pred.ln.logit)) 

# 全てのデータにおける"type"を"No"とする 
pred.logit.type <- rep("No", 200)
# 線形識別関数を用いて0.5より大きい場合を"Yes"とする. 
pred.logit.type[pred.logit >0.5] <- "Yes"
#横軸を真値, 縦軸を予測値としてクロス集計表を作成する.
table( Pima.tr$type, pred.logit.type)
#集計表の対角成分つまり, 的中したデータ数を元に, 誤り率を算出する.
error.rate <- 1-sum(diag(table( Pima.tr$type, pred.logit.type)))/200
error.rate
```



事後確率の値を変化させて, 識別境界を変化させたときのROCのプロット.

```{r logis.roc}
ROC_ <- data.frame(value=pred.logit,answer=Pima.tr$type)
colnames(ROC_) <- c('value','answer')
#出力値順に並べ替える
ROC_ <- ROC_[order(ROC_$value),]

#そのままだと計算が面倒なのでNoを0 Yesを1に変形する
levels(ROC_$answer) <- c(0,1)
ROC_$answer <- ROC_$answer %>% as.character() %>% as.numeric()
table(ROC_$answer)

#　最初にYesの人間が出現するところから
first <- which(ROC_$answer == 1)[1]
ROC_ <- ROC_[first:200,]

# 本当はYesで、閾値をずらしたときに、予測したなかで正しかった割合
true_positive <- cumsum(ROC_$answer)/sum(ROC_$answer)
ROC_$answer <- ROC_$answer -1
false_positive <- cumsum(ROC_$answer) / sum(ROC_$answer)
plot_d <- data.frame(true_positive = true_positive,false_positive = false_positive)
ggplot(plot_d,aes(x=true_positive,y=false_positive))+geom_line()+theme_bw()

```




## 実行例6.4 非線形基底関数による変換とロジスティク回帰による識別


ここでは, 2クラスのロジスティック回帰を用いて非線形基底関数の効用を示す。
実行例6.3で, 2次元判別空間に写像されたアヤメデータをsetosaとvirginicaを１クラスにまとめて線形分離不可能な２クラス問題にし, ロジスティック回帰で求めた識別境界を示す. ロジスティック回帰では, 事後確率が0.5となる等高線が識別境界となる. 図からわかるように, 線形関数では精度良く分類することはできない.

そこで非線形基底関数を使った非線形特徴写像を考える. 非線形特徴写像はいろいろなものが考えられるが, ここでは次のガウス核関数

$$
f(\bf{x}) =\exp ( -\alpha (\bf{x} - \bf{\mu} )^T \bf{\Sigma}^{-1} (\bf{x} - \bf{\mu} )) 
$$

を用いる.


```{r ex6.4}
print(6.4)
```

# サポートベクトルマシン

サポートベクターマシンは, 最大マージンを実現する2クラス線形識別関数の学習法である.

クラスラベル付き学習データの集合を$\mathcal{D}_L = \{ (t_i, \bf{x}_i)\},~(i=1,\ldots, N)$とする. $t_i = \{ -1,1\}$は教師データであり, 学習データ$\bf{x} \in \mathcal{R}^d$がどちらのクラスに属するのかを指定する. 

線形識別関数のマージンを$\kappa$とすれば, すべての学習データで

$$
| \bf{w}^T \bf{x}_i + b| \geq \kappa
$$

が成り立つ. 係数ベクトルとバイアス項をマージン$\kappa$で正規化したものを改めて$\bf{w}$と$b$と置けば, 線形識別関数は
$$
\begin{aligned}
t_i = +1~\textrm{の場合}, ~~~\bf{w}^T \bf{x}_i  + b \geq 1, \\
t_i = -1~\textrm{の場合}, ~~~\bf{w}^T \bf{x}_i  + b \leq -1
\end{aligned}
$$


となる. この場合分けは$t_i( \bf{w}^T \bf{x}_i + b ) \geq 1$とまとめることができる. クラス間マージンは, 各クラスのデータを$\bf{w}$の方向へ射影した長さの差の最小値

$$
\rho (\bf{w}, b) = \min_{x \in C_{y =+1} } \frac{\bf{w}^T \bf{x}}{ \Vert  \bf{w} \Vert}
-
\max_{x \in C_{y =-1} } \frac{\bf{w}^T \bf{x}}{ \Vert  \bf{w} \Vert}
= \frac{2}{ \Vert  \bf{w} \Vert}
$$

で与えられる. 最適な超平面の式を$\bf{w}_0^T\bf{x} + b_0=0$とすれば, この超平面は最大クラス間マージン$\rho(\bf{w}_0, b_0) = \max_{w} \rho(\bf{w},b)$を与える. 従って, 最適識別超平面は, $t_i(\bf{w}^T \bf{x}_i+ b)\geq 1$の制約下で, $\bf{w}$のノルムを最小にする解$\bf{w}_0 = \min  \Vert  \bf{w} \Vert$としてもとめることができる.



`Pima.tr`との`glu`と`bmi`を用いた2次元データをSVMで識別してみよう. RBFカーネルを用いた結果を示す.
動径基底関数の広がりを表すパラメータを$\sigma=0.2, 0.4, 0.8, 2.0,4.0$に設定し, $\alpha_i$の上限値を与えるパラメータ$C$の値値を変えたときの訓練誤差(学習データの再代入誤り率)と,  ホールドアウト法による汎化誤差を示している. （教科書と違います）　



## 実行例8.1 ピマ・インディアンデータデのSVMによる識別


```{r ex8.1.a}
## 動径基底関数を作成する. Create a kernel function using the build in rbfdot function
rbf <- rbfdot(sigma=0.1)
rbf

## train a bound constraint support vector machine
pima.model <- ksvm(type~ bmi+glu ,data=Pima.tr,type="C-svc",
                  kernel=rbf,C=10,prob.model=TRUE)

pima.model

## predict mail type on the test set
pred.tr<- predict(pima.model) 
pred.te <- predict(pima.model,Pima.te)

## 誤り率を計算する
1-sum(diag(table ( pred.tr, Pima.tr$type) ))/200

1-sum(diag(table ( pred.te, Pima.te$type) ))/332

plot(pima.model)
```


```{r ex8.1.b}
svm.rbf <-function(sig,Const){
rbf <- rbfdot(sigma=sig)
## train a bound constraint support vector machine
pima.model <- ksvm(type~ bmi+glu ,data=Pima.tr,type="C-svc",
                  kernel=rbf,C=Const,prob.model=TRUE)
pred.tr<- predict(pima.model) 
pred.te <- predict(pima.model,Pima.te)
## 誤り率を計算する
error.tr <- 1-sum(diag(table ( pred.tr, Pima.tr$type) ))/200
error.te <- 1-sum(diag(table ( pred.te, Pima.te$type) ))/332
return(list(error.te=error.te, error.tr = error.tr))
}

svm.rbf(sig=0.4,Const=1)
```

$\sigma$や$C$を変えながら訓練誤差と汎化誤差の振る舞い振いを調べる.

```{r ex8.1.c}
Sig <- c(0.2,0.4,0.8,4)
Const <- c(1,10, 100,1000,10000)

out.s1 <- sapply(Const,svm.rbf, sig=Sig[1] )
out.s2 <- sapply(Const,svm.rbf, sig=Sig[2] )
out.s3 <- sapply(Const,svm.rbf, sig=Sig[3] )
out.s4 <- sapply(Const,svm.rbf, sig=Sig[4] )
result <- rbind(out.s1,out.s2,out.s3,out.s4)

matplot(0:4,t(result), type="o", xlab="log10(C))", ylab="error rate")
```


$\sigma$の値が小さくなるとカーネル関数が狭くなり, 個々のサポートベクトルが独立に識別境界を決めるようになるので, 複雑な識別境界を構成できるようになる. その半面, 学習データに強く適合してしまうため, 汎化誤差は大きい

$C$の値が大きくなるほど, $\alpha_i$の値が大きくなり, 識別関数の線形結合係数が大きな値を取れるので, 複雑な識別局面が構成できてしまう結果, やはり学習データに強く適合するため汎化誤差が大きくなる. 汎化誤差が一番小さな$C=10, \sigma=0.4$, $C=100, \sigma=0.2$が最適なパラメータであろう. 

汎化誤差が最小となった場合と, 訓練誤差が最小になった場合のサポートベクトルの分布を図に示す.


```{r ex8.1.d}
rbf <- rbfdot(sigma=0.4)
pima.model.test <- ksvm(type~ bmi+glu ,data=Pima.tr,type="C-svc",
                  kernel=rbf,C=10,prob.model=TRUE)
plot(pima.model.test,data= Pima.te)
table( predict(pima.model.test, Pima.te), Pima.te$type)

rbf <- rbfdot(sigma=2)
pima.model.train <- ksvm(type~ bmi+glu ,data=Pima.tr,type="C-svc",
                  kernel=rbf,C=10000,prob.model=TRUE)
plot(pima.model.train,data=Pima.tr)
table( predict(pima.model.train, Pima.tr), Pima.tr$type)
```


